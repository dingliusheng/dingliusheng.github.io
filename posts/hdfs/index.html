<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <title>hdfs学习文档 - 一个技术猿的自留地</title><meta name="author" content="Tincat: ">
<meta name="description" content="全面介绍了hdfs是什么？怎么用？什么原理？" />
<meta name="keywords" content="hdfs" /><meta itemprop="name" content="hdfs学习文档">
<meta itemprop="description" content="全面介绍了hdfs是什么？怎么用？什么原理？"><meta itemprop="datePublished" content="2021-04-15T10:19:14+08:00" />
<meta itemprop="dateModified" content="2021-04-15T10:19:14+08:00" />
<meta itemprop="wordCount" content="5288"><meta itemprop="image" content="https://dingliusheng.github.io/images/homepage_icon.svg"/>
<meta itemprop="keywords" content="hdfs," /><meta property="og:title" content="hdfs学习文档" />
<meta property="og:description" content="全面介绍了hdfs是什么？怎么用？什么原理？" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dingliusheng.github.io/posts/hdfs/" /><meta property="og:image" content="https://dingliusheng.github.io/images/homepage_icon.svg"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-15T10:19:14+08:00" />
<meta property="article:modified_time" content="2021-04-15T10:19:14+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://dingliusheng.github.io/images/homepage_icon.svg"/>

<meta name="twitter:title" content="hdfs学习文档"/>
<meta name="twitter:description" content="全面介绍了hdfs是什么？怎么用？什么原理？"/>
<meta name="application-name" content="一个技术猿的自留地">
<meta name="apple-mobile-web-app-title" content="一个技术猿的自留地"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="canonical" href="https://dingliusheng.github.io/posts/hdfs/" /><link rel="prev" href="https://dingliusheng.github.io/posts/hadoop%E5%85%A5%E9%97%A8/" /><link rel="next" href="https://dingliusheng.github.io/posts/%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95%E8%AE%BA/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "hdfs学习文档",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/dingliusheng.github.io\/posts\/hdfs\/"
    },"genre": "posts","keywords": "hdfs","wordcount":  5288 ,
    "url": "https:\/\/dingliusheng.github.io\/posts\/hdfs\/","datePublished": "2021-04-15T10:19:14+08:00","dateModified": "2021-04-15T10:19:14+08:00","publisher": {
      "@type": "Organization",
      "name": "Tincat"},"author": {
        "@type": "Person",
        "name": "Tincat"
      },"description": "全面介绍了hdfs是什么？怎么用？什么原理？"
  }
  </script></head>
  <body header-desktop="sticky" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : '' === 'dark')) && document.body.setAttribute('theme', 'dark');</script><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="一个技术猿的自留地"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/images/homepage_icon.svg"
    data-srcset="/images/homepage_icon.svg, /images/homepage_icon.svg 1.5x, /images/homepage_icon.svg 2x"
    data-sizes="auto"
    alt="一个技术猿的自留地"
    title="一个技术猿的自留地" /><span class="header-title-text">Tincat</span></a></div>
    <nav>
      <ul class="menu"><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/posts/"
                title="满满的干货🎈"
                
              ><i class='fa fa-book' aria-hidden='true'></i> 文章</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class='fa fa-th-large' aria-hidden='true'></i> 分类</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class='fa fa-tag' aria-hidden='true'></i> 标签</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fas fa-search fa-fw"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fas fa-times-circle fa-fw"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fas fa-spinner fa-fw fa-spin"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fas fa-adjust fa-fw"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="一个技术猿的自留地"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/images/homepage_icon.svg"
    data-srcset="/images/homepage_icon.svg, /images/homepage_icon.svg 1.5x, /images/homepage_icon.svg 2x"
    data-sizes="auto"
    alt="/images/homepage_icon.svg"
    title="/images/homepage_icon.svg" /><span class="header-title-text">Tincat</span></a></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fas fa-search fa-fw"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fas fa-times-circle fa-fw"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fas fa-spinner fa-fw fa-spin"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/posts/"
                title="满满的干货🎈"
                
              ><i class='fa fa-book' aria-hidden='true'></i> 文章</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/categories/"
                
                
              ><i class='fa fa-th-large' aria-hidden='true'></i> 分类</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/tags/"
                
                
              ><i class='fa fa-tag' aria-hidden='true'></i> 标签</a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fas fa-adjust fa-fw"></i>
        </li></ul>
    </nav>
  </div>
</header>
<div class="search-dropdown desktop">
  <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
  <div id="search-dropdown-mobile"></div>
</div>
<main class="container" page-style="narrow"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录</h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    
  </aside>

  <article class="page single"><h1 class="single-title animate__animated animate__flipInX">hdfs学习文档</h1><h2 class="single-subtitle">全面介绍了hdfs是什么？怎么用？什么原理？</h2><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><img
    class="lazyload avatar"
    src="/svg/loading.min.svg"
    data-src="/images/avatar.jpeg"
    data-srcset="/images/avatar.jpeg, /images/avatar.jpeg 1.5x, /images/avatar.jpeg 2x"
    data-sizes="auto"
    alt="Tincat"
    title="Tincat" />&nbsp;Tincat</span></span>
          <span class="post-category">收录于 <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/"><i class="far fa-folder fa-fw"></i>&nbsp;大数据技术</a></span></div>
      <div class="post-meta-line"><span title=2021-04-15&#32;10:19:14>
            <i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021/04/15" >2021/04/15</time>
          </span>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 5288 字&nbsp;
        <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 11 分钟&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp5.itc.cn%2Fimages01%2F20210413%2F922ffb02e6e64fe5b55338d595688d4c.png&amp;refer=http%3A%2F%2Fp5.itc.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1655563949&amp;t=9a80396a1db7660ccfddf052a987b085"
    data-srcset="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp5.itc.cn%2Fimages01%2F20210413%2F922ffb02e6e64fe5b55338d595688d4c.png&amp;refer=http%3A%2F%2Fp5.itc.cn&amp;app=2002&amp;size=f9999%2c10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1655563949&amp;t=9a80396a1db7660ccfddf052a987b085, https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp5.itc.cn%2Fimages01%2F20210413%2F922ffb02e6e64fe5b55338d595688d4c.png&amp;refer=http%3A%2F%2Fp5.itc.cn&amp;app=2002&amp;size=f9999%2c10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1655563949&amp;t=9a80396a1db7660ccfddf052a987b085 1.5x, https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp5.itc.cn%2Fimages01%2F20210413%2F922ffb02e6e64fe5b55338d595688d4c.png&amp;refer=http%3A%2F%2Fp5.itc.cn&amp;app=2002&amp;size=f9999%2c10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1655563949&amp;t=9a80396a1db7660ccfddf052a987b085 2x"
    data-sizes="auto"
    alt="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp5.itc.cn%2Fimages01%2F20210413%2F922ffb02e6e64fe5b55338d595688d4c.png&amp;refer=http%3A%2F%2Fp5.itc.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1655563949&amp;t=9a80396a1db7660ccfddf052a987b085"
    title="全面介绍了hdfs是什么？怎么用？什么原理？" /></div><div class="details toc" id="toc-static" kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fas fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#一hdfs概述">一、HDFS概述</a>
      <ul>
        <li><a href="#11hdfs产生背景">1.1、HDFS产生背景</a></li>
        <li><a href="#12hdfs概念">1.2、HDFS概念</a></li>
        <li><a href="#13hdfs优缺点">1.3、HDFS优缺点</a>
          <ul>
            <li><a href="#131hdfs优点">1.3.1、HDFS优点</a></li>
            <li><a href="#132hdfs缺点">1.3.2、HDFS缺点</a></li>
          </ul>
        </li>
        <li><a href="#14hdfs组成架构">1.4、HDFS组成架构</a></li>
        <li><a href="#15文件块大小">1.5、文件块大小</a></li>
      </ul>
    </li>
    <li><a href="#二hdfs的shell操作开发重点">二、HDFS的Shell操作（开发重点）</a>
      <ul>
        <li><a href="#21-基本语法">2.1 基本语法</a></li>
        <li><a href="#22常用命令">2.2、常用命令</a>
          <ul>
            <li><a href="#221上传">2.2.1、上传</a></li>
            <li><a href="#222下载">2.2.2、下载</a></li>
            <li><a href="#223--hdfs直接操作">2.2.3  HDFS直接操作</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#三hdfs读写流程">三、HDFS读写流程</a>
      <ul>
        <li><a href="#31hdfs写数据流程">3.1、HDFS写数据流程</a></li>
        <li><a href="#32网络拓扑-节点距离计算">3.2、网络拓扑-节点距离计算</a></li>
        <li><a href="#33机架感知副本存储节点选择">3.3、机架感知（副本存储节点选择）</a></li>
        <li><a href="#34hdfs读数据流程">3.4、HDFS读数据流程</a></li>
      </ul>
    </li>
    <li><a href="#三namenode-和-secondarynamenode">三、NameNode <strong>和</strong> SecondaryNameNode</a>
      <ul>
        <li><a href="#31nn和2nn工作机制">3.1、NN和2NN工作机制</a></li>
        <li><a href="#32fsimage-和edits">3.2、Fsimage 和Edits</a></li>
      </ul>
    </li>
    <li><a href="#四datanode工作机制">四、DataNode工作机制</a>
      <ul>
        <li><a href="#41datanode-工作机制">4.1、DataNode 工作机制</a></li>
        <li><a href="#42数据完整性">4.2、数据完整性</a></li>
        <li><a href="#44掉线时限参数设置">4.4、<strong>掉线时限参数设置</strong></a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><h1 id="hdfs">HDFS</h1>
<h2 id="一hdfs概述">一、HDFS概述</h2>
<h3 id="11hdfs产生背景">1.1、HDFS产生背景</h3>
<p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要==一种系统来管理多台机器上的文件==，这就是==分布式文件管理系统==。==HDFS 只是分布式文件管理系统中的一种。==</p>
<h3 id="12hdfs概念">1.2、HDFS概念</h3>
<p>HDFS（Hadoop Distributed File System），它是一个<strong>文件系统</strong>，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS 的<strong>使用场景</strong>：<strong>适合一次写入，多次读出的场景</strong>。一个文件经过创建、写入和关闭之后就不需要改变。</p>
<h3 id="13hdfs优缺点">1.3、HDFS优缺点</h3>
<h4 id="131hdfs优点">1.3.1、HDFS优点</h4>
<ol>
<li>
<p>高容错性</p>
<p>数据会备份多个副本（默认备份3份），当其中一份数据副本丢失，HDFS会自动恢复备份。</p>
</li>
<li>
<p>适合处理大数据</p>
<p>​	数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；</p>
<p>​    文件规模：能够处理百万规模以上的文件数量，数量相当之大。</p>
</li>
<li>
<p>）==可构建在廉价机器上==，通过多副本机制，提高可靠性。</p>
</li>
</ol>
<h4 id="132hdfs缺点">1.3.2、HDFS缺点</h4>
<ol>
<li>
<p><strong>不适合低延时数据访问</strong>， 像mysql快速的增删改查</p>
</li>
<li>
<p><strong>无法高效的对大量小文件进行存储。</strong></p>
<p>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；</p>
<p>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</p>
</li>
<li>
<p><strong>不支持并发写入、文件随机修改。</strong></p>
<p>一个文件只能有一个写，不允许多个线程同时写；</p>
<p><strong>仅支持数据append（追加）</strong>，不支持文件的随机修改。</p>
</li>
</ol>
<h3 id="14hdfs组成架构">1.4、HDFS组成架构</h3>
<ol>
<li>
<p><strong>NameNode（nn）</strong>：就是Master，它是一个主管、管理者。</p>
<p>（1）管理HDFS的名称空间；</p>
<p>（2）配置副本策略；</p>
<p>（3）管理数据块（Block）映射信息；</p>
<p>（4）处理客户端读写请求。</p>
</li>
<li>
<p><strong>DataNode</strong>：就是Slave。NameNode下达命令，DataNode执行实际的操作。</p>
</li>
</ol>
<p>（1）存储实际的数据块；</p>
<p>（2）执行数据块的读/写操作。</p>
<ol start="3">
<li>
<p><strong>Client</strong>：就是客户端。</p>
<p>（1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；</p>
<p>（2）与NameNode交互，获取文件的位置信息；</p>
<p>（3）与DataNode交互，读取或者写入数据；</p>
<p>（4）Client提供一些命令来管理HDFS，比如NameNode格式化；</p>
<p>（5）Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；</p>
</li>
<li>
<p><strong>Secondary NameNode（2nn）</strong>：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<p>（1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；</p>
<p>（2）在紧急情况下，可辅助恢复NameNode</p>
</li>
</ol>
<h3 id="15文件块大小">1.5、文件块大小</h3>
<p>HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数( dfs.blocksize）来规定，默认大小在<strong>Hadoop2.x/3.x版本中是==128M==，1.x版本中是==64M==</strong>。</p>
<p><strong>为什么块的大小不能设置太小，也不能设置太大？</strong></p>
<p>（1）<strong>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</strong></p>
<p>​		例如：一个块1kb，一个100kb的文件需要切成100块，读取这个文件时需要找到这100个块，寻找数据块的时间大大增加</p>
<p>（2）<strong>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢</strong></p>
<p>​	例如：一个块1GB，一个100kb的文件存储，读取文件时需要在1GB 磁盘里读取数据（相当于读取1GB的数据量）。</p>
<h2 id="二hdfs的shell操作开发重点">二、HDFS的Shell操作（开发重点）</h2>
<h3 id="21-基本语法">2.1 基本语法</h3>
<ul>
<li>​	hadoop  fs  具体命令</li>
<li>（或者）hdfs   dfs  具体命令</li>
</ul>
<h3 id="22常用命令">2.2、常用命令</h3>
<h4 id="221上传">2.2.1、上传</h4>
<ol>
<li>-moveFromLocal：从本地剪切粘贴到 HDFS</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -moveFromLocal 本地文件路径   HDFS文件路径
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs -moveFromLocal ./test1.txt /output
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>
<p>-copyFromLocal:从本地文件系统中拷贝文件到 HDFS 路径去</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -copyFromLocal 本地文件路径   HDFS文件路径
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs -copyFromLocal ./test2.txt /output
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>-put：等同于 copyFromLocal，==生产环境更习惯用 put==</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -put 本地文件路径   HDFS文件路径
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs -put ./test3.txt /output
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>-appendToFile：追加一个文件到已经存在的文件末尾</p>
</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs  -appendToFile 本地文件路径   HDFS文件路径
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs  -appendToFile ./test4.txt /output/test3.txt 
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="222下载">2.2.2、下载</h4>
<p>-copyToLocal：从 HDFS 拷贝到本地</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs  -copyToLocal    HDFS文件路径     本地文件路径
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs  -copyToLocal  /output/test3.txt    ./
</span></span></code></pre></td></tr></table>
</div>
</div><p>-get：等同于 copyToLocal，==生产环境更习惯用 get==</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs  -put    HDFS文件路径     本地文件路径
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs  -put  /output/test3.txt    ./
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="223--hdfs直接操作">2.2.3  HDFS直接操作</h4>
<p>-ls: 显示目录信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs  -ls    HDFS文件路径 
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs  -ls    /
</span></span></code></pre></td></tr></table>
</div>
</div><p>-cat：显示文件内容</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs  -cat    HDFS文件路径 
</span></span><span class="line"><span class="cl"><span class="c1">#举例</span>
</span></span><span class="line"><span class="cl">hadoop fs  -cat    /output/test3.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p>-chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"> hadoop fs -chmod <span class="m">666</span>   /output/test3.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p>-mkdir：创建文件夹</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -mkdir  /output/aaa
</span></span></code></pre></td></tr></table>
</div>
</div><p>-cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"> hadoop fs -cp   /output/test3.txt   /tmp
</span></span></code></pre></td></tr></table>
</div>
</div><p>-mv：在 HDFS 目录中移动文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">hadoop fs -cp   /tmp/test3.txt    /output/aaa   
</span></span></code></pre></td></tr></table>
</div>
</div><p>-tail：显示一个文件的末尾 1kb 的数据</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"> hadoop fs -tail   /output/test3.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p>-rm：删除文件或文件夹</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -rm   /tmp/test3.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p>-rm -r：递归删除目录及目录里面内容</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -rm  -r /tmp
</span></span></code></pre></td></tr></table>
</div>
</div><p>-du 统计文件夹的大小信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hadoop fs -du -h /output
</span></span><span class="line"><span class="cl">hadoop fs -du -s /output
</span></span></code></pre></td></tr></table>
</div>
</div><p>-setrep：设置 HDFS 中文件的副本数量</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"> hadoop fs -setrep <span class="m">10</span>  /output/test3.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里设置的副本数只是记录在 NameNode 的元数据中，是否真的会有这么多副本，还得看 DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10台时，副本数才能达到 10</p>
<h2 id="三hdfs读写流程">三、HDFS读写流程</h2>
<h3 id="31hdfs写数据流程">3.1、HDFS写数据流程</h3>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101719756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101719756.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101719756.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101719756.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101719756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<ol>
<li>
<p>客户端向NameNode 请求上传文件  /usr/data/data1</p>
</li>
<li>
<p>NameNode 检查客户端有没有权限写入数据，检查目录  文件是否已经存在 ，如果有权限且文件不存在，则响应客户端可以上传文件</p>
</li>
<li>
<p>客户端请求上传第一个block(0~128M 数据块) ，请返回DataNode</p>
</li>
<li>
<p>NameNode 通过节点距离计算和负载均衡得出存储的节点，返回存放数据的3个DataNode 节点  dn1、dn2、dn3</p>
</li>
<li>
<p>客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>
</li>
<li>
<p>dn1、dn2、dn3 逐级应答客户端,通知客户端可以传数据</p>
</li>
<li>
<p>客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位（大小64k），dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet会放入一个应答队列等待应答</p>
</li>
<li>
<p>当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行 3-7 步）。</p>
</li>
</ol>
<h3 id="32网络拓扑-节点距离计算">3.2、网络拓扑-节点距离计算</h3>
<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接</p>
<p>收数据。那么这个最近距离怎么计算呢？</p>
<p>节点距离：<strong>两个节点到达最近的共同祖先的距离总和</strong>。</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101706562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101706562.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101706562.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101706562.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101706562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<p>理解： 节点1&mdash;&mdash;机架1&mdash;&mdash;集群1&mdash;数据总部 ，走过3个节点</p>
<p>​			节点2&mdash;&mdash;机架2&mdash;&mdash;集群2&mdash;&mdash;数据总部 ， 走过3个节点，那么节点1和节点2距离为6</p>
<h3 id="33机架感知副本存储节点选择">3.3、机架感知（副本存储节点选择）</h3>
<ul>
<li>
<p>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。</p>
</li>
<li>
<p>第二个副本在另一个机架的随机一个节点。</p>
</li>
<li>
<p>第三个副本在第二个副本所在机架的随机节点。</p>
</li>
</ul>
<h3 id="34hdfs读数据流程">3.4、HDFS读数据流程</h3>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101756185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101756185.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101756185.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101756185.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101756185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<ol>
<li>客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</li>
<li>挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。</li>
<li>客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h2 id="三namenode-和-secondarynamenode">三、NameNode <strong>和</strong> SecondaryNameNode</h2>
<h3 id="31nn和2nn工作机制">3.1、NN和2NN工作机制</h3>
<p>思考：NameNode 中的元数据是存储在哪里的？</p>
<p>首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生<strong>在磁盘中备份元数据的FsImage</strong>。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。**因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中。**这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，<strong>引入一个新的节点SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101738369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101738369.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101738369.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101738369.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101738369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<p><strong>第一阶段： NameNode 启动</strong></p>
<ol>
<li>第一次启动 namenode 格式化后，创建 fsimage 和 edits （在 namenode 所在结点的 hadooop/data 目录下）文件。如果不是第一次启动，直接加载编辑日志edits_inprogress_001和镜像文件fsimage 到内存。</li>
<li>客户端对元数据进行增删改的请求</li>
<li>namenode 记录操作日志edits_inprogress_001</li>
<li>namenode 在内存中对数据进行删改</li>
</ol>
<p><strong>第二阶段： Secondary NameNode 工作</strong></p>
<ol>
<li>Secondary NameNode 询问 namenode 是否需要 checkpoint 。直接带回 namenode 是否检查结果。</li>
<li>Secondary NameNode 请求执行 checkpoint 。</li>
<li>namenode 滚动正在写的 edits 日志（将正在使用编辑的编辑日志edits_inprogress_001改名edits_001，并使用edits_inprogress_002作为记录操作新的编辑日志）</li>
<li>将滚动前的编辑日志edits_001和镜像文件fsimage 拷贝到 Secondary NameNode</li>
<li>Secondary NameNode 加载编辑日志和镜像文件fsimage 到内存，并合并。</li>
<li>生成新的镜像文件 fsimage.chkpoint</li>
<li>拷贝 fsimage.chkpoint 到 namenode</li>
<li>namenode 将 fsimage.chkpoint 重新命名成 fsimage，原先的fsimage作为历史文件保留</li>
</ol>
<h3 id="32fsimage-和edits">3.2、Fsimage 和Edits</h3>
<p>NameNode被格式化之后，将在/opt/module/hadoop-3.1.3/data/tmp/dfs/name/current目录中产生如下文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">fsimage_0000000000000000000
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">fsimage_0000000000000000000.md5
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">seen_txid
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">VERSION
</span></span></code></pre></td></tr></table>
</div>
</div><p>（1）Fsimage文件：HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目</p>
<p>​									录和文件inode的序列化信息。</p>
<p>（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先</p>
<p>​								会被记录到Edits文件中。</p>
<p>（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字</p>
<p>（4）每 次NameNode启动的时候都会将Fsimage文件读入内存，加 载Edits里面的更新操作，保证内存</p>
<p>中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合				并。</p>
<h2 id="四datanode工作机制">四、DataNode工作机制</h2>
<h3 id="41datanode-工作机制">4.1、DataNode 工作机制</h3>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101828667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101828667.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101828667.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101828667.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101828667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<ol>
<li>
<p>一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是<strong>数据本身</strong>，一个是<strong>元数据</strong>包括数据块的长度，块数据的校验和，以及时间戳。</p>
</li>
<li>
<p>DataNode 启动后向 NameNode 注册，通过后，周期性（6 小时）的向 NameNode 上报所有的块信息。（向NameNode汇报文件状态有没有损毁）</p>
<p>​	DN 向 NN 汇报当前解读信息的时间间隔，默认 6 小时；</p>
<p>​	DN 扫描自己节点块信息列表的时间，默认 6 小时</p>
</li>
<li>
<p>心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块数据到另一台机器，或删除某个数据块。如果超过 10 分钟没有收到某个 DataNode 的心跳，则认为该节点不可用。（向NameNode汇报DataNode的状态，是否挂掉，如果10分钟内没有反馈，则认为DataNode 不存在，不会再和这个DataNode通信）</p>
</li>
</ol>
<h3 id="42数据完整性">4.2、数据完整性</h3>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101818848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101818848.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101818848.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101818848.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101818848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<p>奇偶校验：</p>
<p>数据： 0100 0001  1的个数是2  是偶数  奇偶校验位为0</p>
<p>收到的传输数据进行奇偶校验，如果1的个数奇偶和奇偶校验位一致，则认为数据是完整的</p>
<p>hadoop 采用 crc（32），常见的校验算法还有 md5（128），sha1（160）</p>
<p>都是对数据按一定算法得出的结果和校验位进行比较，判断数据是否完整</p>
<h3 id="44掉线时限参数设置">4.4、<strong>掉线时限参数设置</strong></h3>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://img-blog.csdnimg.cn/20210415101840208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    data-srcset="https://img-blog.csdnimg.cn/20210415101840208.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center, https://img-blog.csdnimg.cn/20210415101840208.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 1.5x, https://img-blog.csdnimg.cn/20210415101840208.png?x-oss-process=image/watermark%2ctype_ZmFuZ3poZW5naGVpdGk%2cshadow_10%2ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3%2csize_16%2ccolor_FFFFFF%2ct_70#pic_center 2x"
    data-sizes="auto"
    alt="https://img-blog.csdnimg.cn/20210415101840208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE2MDI3,size_16,color_FFFFFF,t_70#pic_center"
    title="在这里插入图片描述" /></p>
<p>如果定义超时时间为TimeOut，则超时时长的计算公式为：</p>
<p>TimeOut = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。</p>
<p>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p>
<p>hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-xml" data-lang="xml"><span class="line"><span class="cl"><span class="nt">&lt;property&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;name&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="nt">&lt;/name&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;value&gt;</span>300000<span class="nt">&lt;/value&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nt">&lt;/property&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nt">&lt;property&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;name&gt;</span>dfs.heartbeat.interval<span class="nt">&lt;/name&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nt">&lt;/property&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>dfs.heartbeat.interval。</p>
<p>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p>
<p>hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-xml" data-lang="xml"><span class="line"><span class="cl"><span class="nt">&lt;property&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;name&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="nt">&lt;/name&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;value&gt;</span>300000<span class="nt">&lt;/value&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nt">&lt;/property&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nt">&lt;property&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;name&gt;</span>dfs.heartbeat.interval<span class="nt">&lt;/name&gt;</span>
</span></span><span class="line"><span class="cl"> 	<span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nt">&lt;/property&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2021-04-15&#32;10:19:14>更新于 2021/04/15</span>
      </div>
      <div class="post-info-license"></div>
    </div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a
  href="/posts/hdfs/index.md"
  
    title="阅读原始文档"
  
  
  
  
  
    class="link-to-markdown"
  
  
>阅读原始文档</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="https://dingliusheng.github.io/posts/hdfs/" data-title="hdfs学习文档" data-description="全面介绍了hdfs是什么？怎么用？什么原理？"><i class="fab fa-blogger fa-fw"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/hdfs/">hdfs</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/posts/hadoop%E5%85%A5%E9%97%A8/" class="prev" rel="prev" title="hadoop入门文档"><i class="fas fa-angle-left fa-fw"></i>hadoop入门文档</a>
      <a href="/posts/%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95%E8%AE%BA/" class="next" rel="next" title="维度建模方法论">维度建模方法论<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line custom">一个技术猿的自留地</div><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer" title="Hugo 0.98.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/Lruihao/FixIt" target="_blank" rel="external nofollow noopener noreffer" title="FixIt v0.2.14-RC"><img class="fixit-icon" src="/images/fixit.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright"><i class="far fa-copyright fa-fw"></i>
            <span itemprop="copyrightYear">2022</span><span class="author" itemprop="copyrightHolder">
              <a
  href="/"
  
  
  
  
  
  
>Tincat</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fas fa-heartbeat fa-fw animate-icon"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div></div>
  </footer></div><div class="widgets">
  <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
      <i class="fas fa-arrow-up fa-fw"></i>
    </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
      <i class="fas fa-comment fa-fw"></i>
    </a>
  </div><div id="mask"></div>
</div><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js" defer></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js" async defer></script><script type="text/javascript" src="/lib/sharer/sharer.min.js" async defer></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{},"enablePWA":null,"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"siteTime":"2022-05-15T19:30:34+08:00"};</script><script type="text/javascript" src="/js/theme.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/jquery-backstretch@2.1.18/jquery.backstretch.min.js"></script><script type="text/javascript" src="/js/custom.js"></script></body>
</html>
